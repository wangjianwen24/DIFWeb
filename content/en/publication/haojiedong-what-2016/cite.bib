@article{HaojiedongWhat2016,
 abstract = {Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion about the best combination of them. The main contribution of our paper is the thorough evaluations of the various factors that affect the discriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify the best choices for different factors and propose a new multi-scale image feature representation method to encode the image effectively. Finally, we show that the proposed method generalises well and outperforms the state-of-the-art methods on four typical datasets used for visual instance retrieval.},
 archiveprefix = {arXiv},
 arxivid = {1611.01640},
 author = {Hao, Jiedong and Dong, Jing and Wang, Wei and Tan, Tieniu},
 eprint = {1611.01640},
 journal = {arXiv preprint arXiv:1611.01640},
 month = {nov},
 title = {What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?},
 url = {http://arxiv.org/abs/1611.01640},
 year = {2016}
}

